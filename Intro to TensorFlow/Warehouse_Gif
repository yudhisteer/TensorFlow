{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" Warehouse_Gif","provenance":[{"file_id":"1oYo2oJvwCUp7i9yljS2qRz3tSLwvm8ix","timestamp":1607109845029}],"collapsed_sections":[],"authorship_tag":"ABX9TyOyYwZ1+/0nNVdTb8Dpk+Jh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"D6-jA4yHJdiT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611041952127,"user_tz":-240,"elapsed":5647,"user":{"displayName":"Yudhisteer Chintaram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzuFN9vAt6zvN9YC7DIyWW-wHiT2Ap5HtGwikgeQ=s64","userId":"14390973636838354764"}},"outputId":"9a8056fe-770f-4160-c0fb-546909433d49"},"source":["pip install -U gif"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: gif in /usr/local/lib/python3.6/dist-packages (3.0.0)\n","Requirement already satisfied, skipping upgrade: Pillow>=7.1.2 in /usr/local/lib/python3.6/dist-packages (from gif) (8.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qrjEUuwlJFan","executionInfo":{"status":"ok","timestamp":1611041952130,"user_tz":-240,"elapsed":5634,"user":{"displayName":"Yudhisteer Chintaram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzuFN9vAt6zvN9YC7DIyWW-wHiT2Ap5HtGwikgeQ=s64","userId":"14390973636838354764"}}},"source":["import matplotlib.pyplot as plt\n","from matplotlib import colors\n","import gif\n","import numpy as np\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3F1rTgILJNMZ","executionInfo":{"status":"ok","timestamp":1611042000527,"user_tz":-240,"elapsed":54024,"user":{"displayName":"Yudhisteer Chintaram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzuFN9vAt6zvN9YC7DIyWW-wHiT2Ap5HtGwikgeQ=s64","userId":"14390973636838354764"}},"outputId":"bd1d7050-f129-4f94-c96b-b0e1e3500b20"},"source":["import matplotlib.pyplot as plt\n","from matplotlib import colors\n","import gif\n","import numpy as np\n","\n","\n","### SETTINGS###\n","MAX_ROUTING_STEPS = 12\n","\n","# Setting parameters gamma and alpha for Q-learning\n","gamma = 0.75\n","alpha = 0.9\n","\n","# >>PART 1 - DEFINING THE ENVIRONMENT<<\n","\n","# i) Defining the states\n","\n","location_to_state = {\n","    \"A\": 0,\n","    \"B\": 1,\n","    \"C\": 2,\n","    \"D\": 3,\n","    \"E\": 4,\n","    \"F\": 5,\n","    \"G\": 6,\n","    \"H\": 7,\n","    \"I\": 8,\n","    \"J\": 9,\n","    \"K\": 10,\n","    \"L\": 11,\n","}\n","\n","\n","# ii) Defining the actions\n","\n","actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","\n","# iii) Defining the rewards\n","\n","R = np.array(\n","    [\n","        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","        [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n","        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n","        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n","        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n","        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n","    ]\n",")\n","\n","columns = 4\n","rows = 3\n","\n","\n","### END SETTINGS ###\n","\n","\n","def optimal_route(starting_location, ending_location, Q):\n","    \"\"\"Find the optimal route.\n","    It can happen that it will have a loop,\n","    so we add a counter and terminate when it needs too many steps.\"\"\"\n","    count = 0\n","    route = [starting_location]\n","    next_location = starting_location\n","    while next_location != ending_location:\n","        if count > MAX_ROUTING_STEPS:\n","            break\n","        starting_state = location_to_state[starting_location]\n","        next_state = np.argmax(\n","            Q[\n","                starting_state,\n","            ]\n","        )  # we getting  actions that will give us highest Q-value in the row of starting_state\n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        starting_location = next_location\n","        count = count + 1\n","    return route\n","\n","\n","def create_gif(\n","    b_route, starting_location, ending_location, intermediary=None, iterations=None\n","):\n","    \"\"\"For every run we create a gif. We do this by passing the route and the location\"\"\"\n","    start = location_to_state[starting_location]\n","    finish = location_to_state[ending_location]\n","    if intermediary is not None:\n","        intermediary = location_to_state[intermediary]\n","\n","    frames = []\n","\n","    for current in b_route:\n","        frames.append(\n","            draw_map(\n","                start,\n","                finish,\n","                intermediary,\n","                location_to_state[current],\n","                rows,\n","                columns,\n","                start_name=starting_location,\n","                end_name=ending_location,\n","                iterations=iterations,\n","            )\n","        )\n","\n","    return frames\n","\n","\n","def get_yx(i):\n","    x = i % columns\n","    y = int(i / columns)\n","    return y, x\n","\n","\n","@gif.frame\n","def draw_map(\n","    start=0,\n","    finish=11,\n","    intermediary=None,\n","    current=0,\n","    rows=3,\n","    columns=4,\n","    start_name=None,\n","    end_name=None,\n","    iterations=None,\n","):\n","    \"\"\"Draw the map. Here we draw the map, fill the locations and return the frame.\"\"\"\n","\n","    grid = np.zeros((rows, columns))\n","    grid[get_yx(finish)] = 3\n","\n","    if intermediary is None:\n","        intermediary = start\n","    grid[get_yx(intermediary)] = 2\n","    grid[get_yx(start)] = 1\n","\n","    grid[get_yx(current)] = 4\n","\n","    # Adjust here the color codes.\n","    cmap = colors.ListedColormap([\"#add8e6\", \"#ffb6c1\", \"yellow\", \"green\", \"#97CBFF\"])\n","    plt.imshow(grid, cmap=cmap)\n","\n","    for line in lines:\n","        plt.plot(line[0], line[1], \"k-\", lw=2)\n","\n","    for grid in grids:\n","        plt.plot(grid[0], grid[1], \"k-\", lw=0.5)\n","\n","    plt.axis(\"off\")\n","\n","    if iterations is not None:\n","        title = f\"From {start_name} to {end_name} with {iterations} iterations.\"\n","    else:\n","        title = f\"From {start_name} to {end_name}\"\n","    plt.title(title)\n","    plt.draw()\n","\n","\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","\n","# i) Initiliaze the Q values - 12x12 matrix of 0 for Q-values\n","\"\"\"Q = np.array(np.zeros([12,12]))\n","# ii) Implementing the Q-Learning Process\n","for i in range (1000):\n","    current_state = np.random.randint(0,12) # select a random state St\n","    playable_actions = []\n","    for j in range(12): # the 12 columns in matrix R to check for Reward,R>0\n","        if R[current_state,j] >0:\n","            playable_actions.append(j)\n","    next_state = np.random.choice(playable_actions)# play a random action from playable_ations list\n","    TD = R[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state] #action a = next_state, state st = current_state\n","    Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD \"\"\"\n","\n","\n","# PART 3 - GOING INTO PRODUCTION\n","\n","# Make mapping from states to locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","\n","\n","# Making the final function that will return the optimal route\n","def route(starting_location, ending_location):\n","    learning_frames = []\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    # i) Initiliaze the Q values - 12x12 matrix of 0 for Q-values\n","    Q = np.array(np.zeros([12, 12]))\n","    # ii) Implementing the Q-Learning Process\n","    for i in range(1000):\n","        current_state = np.random.randint(0, 12)  # select a random state St\n","        playable_actions = []\n","        for j in range(12):  # the 12 columns in matrix R to check for Reward,R>0\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(\n","            playable_actions\n","        )  # play a random action from playable_ations list\n","        TD = (\n","            R_new[current_state, next_state]\n","            + gamma\n","            * Q[\n","                next_state,\n","                np.argmax(\n","                    Q[\n","                        next_state,\n","                    ]\n","                ),\n","            ]\n","            - Q[current_state, next_state]\n","        )  # action a = next_state, state st = current_state\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","\n","        # create another GIF for intermediate result\n","        if i % 20 == 0:\n","            r = optimal_route(starting_location, ending_location, Q)\n","            learning_frames.extend(\n","                create_gif(r, starting_location, ending_location, iterations=i)\n","            )\n","\n","    gif.save(\n","        learning_frames,\n","        f\"route_leanring_{starting_location}_{ending_location}.gif\",\n","        duration=4,\n","    )\n","\n","    return optimal_route(starting_location, ending_location, Q)\n","\n","\n","def best_route(starting_location, intermediary_location, ending_location):\n","    best = (\n","        route(starting_location, intermediary_location)\n","        + route(intermediary_location, ending_location)[1:]\n","    )\n","    final_frames = create_gif(\n","        best, starting_location, ending_location, intermediary=intermediary_location\n","    )\n","    gif.save(\n","        final_frames,\n","        f\"route_final_{starting_location}_{ending_location}.gif\",\n","        duration=400,\n","    )\n","\n","    return best\n","\n","\n","lines = []\n","\n","\n","def get_coor(i):\n","    x = i % columns\n","    y = int(i / columns)\n","    return x, y\n","\n","\n","for i in range(rows * columns):\n","    # draw vertical\n","    if (i + 1) % columns != 0:\n","        if R[i, i + 1] == 0:\n","            x, y = get_coor(i)\n","            lines.append(([x + 0.5, x + 0.5], [y - 0.5, y + 0.5]))\n","\n","    # draw horizontal\n","    if (np.ceil(i / rows) < rows) & (i + columns < rows * columns):\n","        if R[i, i + columns] == 0:\n","            x, y = get_coor(i)\n","            lines.append(([x - 0.5, x + 0.5], [y + 0.5, y + 0.5]))\n","\n","\n","grids = []\n","\n","for i in range(columns - 1):\n","    grids.append(([i + 0.5, i + 0.5], [-0.5, rows - 0.5]))\n","for i in range(rows - 1):\n","    grids.append(([-0.5, columns - 0.5], [i + 0.5, i + 0.5]))\n","\n","b_route = best_route(\"E\", \"D\", \"G\")\n","\n","print(b_route)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["['E', 'I', 'J', 'K', 'L', 'H', 'D', 'H', 'G']\n"],"name":"stdout"}]}]}